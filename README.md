Image caption generation is a critical research area
that combines computer vision and natural language processing,
with wide-ranging implications such as assisting visually impaired
individuals, improving autonomous vehicle capabilities, and refin-
ing image search algorithms. The main goal of image captioning
is to identify objects in an image and explain the relationships
between them using text. In the context of image captioning,
this research compares three popular encoding architectures:
ResNet50, VGG16 and InceptionV3. To analyse the input image
in order to obtain significant features that are then converted for
vector representation, an Encoder component of the architecture
is used. These encoded features are used by the decoder module
to create a coherent text description using the Long Short Term
Memory LSTM model. By conducting thorough experiments
and evaluations, this study aims to uncover the performance
differences among the mentioned encoder architectures and
determine the best model for image captioning tasks.

Refer the pdf file attached for better understanding and result illustrations.

![image](https://github.com/dshree-21/image-captioner-analysis/assets/90754321/acef49c1-0be2-4e60-a548-a61028abbb7f)
